{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc96097",
   "metadata": {},
   "source": [
    "# Global Research Keyword Analysis (Refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7becfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium import plugins\n",
    "import ast\n",
    "from collections import Counter\n",
    "from IPython.display import IFrame, display\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "import requests\n",
    "\n",
    "# Load dataset and drop nulls in critical columns\n",
    "df = pd.read_csv('../FINAL_ARXIV_2025_with_affiliations.csv')\n",
    "df = df.dropna(subset=['affiliations', 'keywords', 'smart_keywords'])\n",
    "\n",
    "coords_df = pd.read_csv('../world_coords.csv')\n",
    "\n",
    "# GeoJSON URL for country boundaries\n",
    "political_countries_url = \"http://geojson.xyz/naturalearth-3.3.0/ne_50m_admin_0_countries.geojson\"\n",
    "\n",
    "plt.style.use('dark_background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69b63973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Country Extraction and Keyword Cleaning\n",
    "def extract_country_from_affil(affil_str):\n",
    "    if not isinstance(affil_str, str) or not affil_str:\n",
    "        return None\n",
    "    \n",
    "    country_map = {\n",
    "        'usa': 'United States',\n",
    "        'united states': 'United States',\n",
    "        'uk': 'United Kingdom',\n",
    "        'united kingdom': 'United Kingdom',\n",
    "        'china': 'China',\n",
    "        \"people's republic of china\": 'China',\n",
    "        'germany': 'Germany',\n",
    "        'france': 'France',\n",
    "        'italy': 'Italy',\n",
    "        'india': 'India',\n",
    "        'japan': 'Japan',\n",
    "        'canada': 'Canada',\n",
    "        'australia': 'Australia',\n",
    "        'spain': 'Spain',\n",
    "        'russia': 'Russia',\n",
    "        'brazil': 'Brazil',\n",
    "        'south korea': 'South Korea',\n",
    "        'switzerland': 'Switzerland',\n",
    "        'netherlands': 'Netherlands',\n",
    "        'sweden': 'Sweden',\n",
    "        'taiwan': 'Taiwan',\n",
    "        'israel': 'Israel',\n",
    "        'austria': 'Austria',\n",
    "        'denmark': 'Denmark',\n",
    "        'belgium': 'Belgium',\n",
    "        'finland': 'Finland',\n",
    "        'norway': 'Norway',\n",
    "        'poland': 'Poland',\n",
    "        'mexico': 'Mexico',\n",
    "        'chile': 'Chile',\n",
    "        'argentina': 'Argentina',\n",
    "        'czech republic': 'Czech Republic',\n",
    "        'turkey': 'Turkey',\n",
    "        'greece': 'Greece',\n",
    "        'portugal': 'Portugal',\n",
    "        'singapore': 'Singapore',\n",
    "        'south africa': 'South Africa',\n",
    "        'hong kong': 'Hong Kong',\n",
    "        'new zealand': 'New Zealand',\n",
    "        'ireland': 'Ireland',\n",
    "        'hungary': 'Hungary',\n",
    "        'colombia': 'Colombia'\n",
    "    }\n",
    "    \n",
    "    parts = [p.strip().lower() for p in affil_str.split(',')]\n",
    "    for part in reversed(parts):\n",
    "        clean_part = re.sub(r'[^a-zA-Z\\s]', '', part).strip()\n",
    "        if clean_part in country_map:\n",
    "            return country_map[clean_part]\n",
    "    \n",
    "    affil_lower = affil_str.lower()\n",
    "    for key, val in country_map.items():\n",
    "        if key in affil_lower:\n",
    "            return val\n",
    "            \n",
    "    return None\n",
    "\n",
    "def process_affiliations(row):\n",
    "    try:\n",
    "        aff_raw = row['affiliations']\n",
    "        affs = aff_raw.split(';') if ';' in aff_raw else [aff_raw]\n",
    "        extracted_countries = []\n",
    "        for aff in affs:\n",
    "            country = extract_country_from_affil(aff)\n",
    "            if country:\n",
    "                extracted_countries.append(country)\n",
    "        return list(set(extracted_countries))\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def clean_keyword(k):\n",
    "    # Remove things like \"(573)\" from \"Galaxies (573)\"\n",
    "    s = re.sub(r'\\s*\\(\\d+\\)\\s*$', '', str(k)).strip()\n",
    "    # Filter out email addresses\n",
    "    if '@' in s:\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "df['countries_extracted'] = df.apply(process_affiliations, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd4de132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation and Coloring Functions\n",
    "def get_country_top_keywords(df, col_name):\n",
    "    country_keywords = {}\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            countries = row['countries_extracted']\n",
    "            # Parse keywords\n",
    "            raw_keywords = ast.literal_eval(row[col_name])\n",
    "            cleaned_keywords = [clean_keyword(k) for k in raw_keywords if k]\n",
    "            cleaned_keywords = [k for k in cleaned_keywords if k] # Filter None\n",
    "            \n",
    "            if not countries or not cleaned_keywords: continue\n",
    "            \n",
    "            for country in countries:\n",
    "                if country not in country_keywords:\n",
    "                    country_keywords[country] = Counter()\n",
    "                country_keywords[country].update(cleaned_keywords)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    results = {}\n",
    "    all_top_keywords = set()\n",
    "    for country, counts in country_keywords.items():\n",
    "        most_common = counts.most_common(5)\n",
    "        if most_common:\n",
    "            results[country] = most_common\n",
    "            all_top_keywords.add(most_common[0][0])\n",
    "            \n",
    "    return results, sorted(list(all_top_keywords))\n",
    "\n",
    "def create_legend_html(keyword_colors, title):\n",
    "    legend_html = f'''\n",
    "    <div style=\"position: fixed; bottom: 50px; left: 50px; width: 250px; height: auto; \n",
    "                border:2px solid grey; z-index:9999; font-size:12px;\n",
    "                background-color:white; opacity: 0.9; padding: 10px; max-height: 400px; overflow-y: auto;\">\n",
    "    <b>{title}</b><br>\n",
    "    '''\n",
    "    for kw, color in keyword_colors.items():\n",
    "        legend_html += f'<i style=\"background:{color}; width:12px; height:12px; float:left; margin-right:5px; border: 1px solid black;\"></i>{kw}<br>'\n",
    "    legend_html += '</div>'\n",
    "    return legend_html\n",
    "\n",
    "def generate_keyword_map(data_dict, unique_keywords, title, filename):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2.3, tiles='cartodb positron')\n",
    "    \n",
    "    # Create colormap using 'jet'\n",
    "    jet = cm.get_cmap('jet', len(unique_keywords))\n",
    "    keyword_colors = {kw: colors.to_hex(jet(i)) for i, kw in enumerate(unique_keywords)}\n",
    "    \n",
    "    # GeoJSON with colors\n",
    "    def style_function(feature):\n",
    "        country_name = feature['properties']['name']\n",
    "        mapping = {\n",
    "            'United States': 'United States', 'United States of America': 'United States',\n",
    "            'United Kingdom': 'United Kingdom', 'China': 'China',\n",
    "            'Czech Rep.': 'Czech Republic', 'Dem. Rep. Korea': 'South Korea',\n",
    "            'Korea': 'South Korea'\n",
    "        }\n",
    "        std_name = mapping.get(country_name, country_name)\n",
    "        \n",
    "        kw_info = data_dict.get(std_name)\n",
    "        if kw_info:\n",
    "            top_kw = kw_info[0][0]\n",
    "            return {\n",
    "                'fillColor': keyword_colors.get(top_kw, '#ffffff'),\n",
    "                'color': 'black',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.6\n",
    "            }\n",
    "        return {\n",
    "            'fillColor': '#ffffff',\n",
    "            'color': 'black',\n",
    "            'weight': 1,\n",
    "            'fillOpacity': 0.1\n",
    "        }\n",
    "\n",
    "    folium.GeoJson(\n",
    "        political_countries_url,\n",
    "        style_function=style_function,\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['name'], aliases=['Country:'])\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add Markers (Circles Only, No Text Labels)\n",
    "    for country, top_kws in data_dict.items():\n",
    "        if country in coords_df['country'].values:\n",
    "            lat = coords_df[coords_df['country'] == country]['latitude'].values[0]\n",
    "            lon = coords_df[coords_df['country'] == country]['longitude'].values[0]\n",
    "            \n",
    "            top_kw = top_kws[0][0]\n",
    "            \n",
    "            # folium.CircleMarker(\n",
    "            #     location=[lat, lon],\n",
    "            #     radius=4,\n",
    "            #     color='black',\n",
    "            #     weight=1,\n",
    "            #     fill=True,\n",
    "            #     fill_color=keyword_colors.get(top_kw, 'white'),\n",
    "            #     fill_opacity=1,\n",
    "            #     popup=f\"<b>{country}</b><br>Top Keyword: {top_kw}<br>Total Keywords:<br>\" + \"<br>\".join([f\"{k}: {c}\" for k, c in top_kws])\n",
    "            # ).add_to(m)\n",
    "\n",
    "    # Add Legend\n",
    "    m.get_root().html.add_child(folium.Element(create_legend_html(keyword_colors, title)))\n",
    "    \n",
    "    m.save(filename)\n",
    "    display(IFrame(src=filename, width='100%', height='600px'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543a0aa",
   "metadata": {},
   "source": [
    "## Map 1: Top Keywords per Country (All Authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1f2db50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing keywords column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hetan\\AppData\\Local\\Temp\\ipykernel_23192\\4286491426.py:47: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  jet = cm.get_cmap('jet', len(unique_keywords))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"global_keywords_map.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x23aa4dcc550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate Map 1: Most Frequent Keywords\n",
    "print(\"Processing keywords column...\")\n",
    "kw_results, unique_kws = get_country_top_keywords(df, 'keywords')\n",
    "generate_keyword_map(kw_results, unique_kws, \"Top Research Keywords\", \"global_keywords_map.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e981d",
   "metadata": {},
   "source": [
    "## Map 2: Top Smart Keywords per Country (All Authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6545c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing smart_keywords column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hetan\\AppData\\Local\\Temp\\ipykernel_23192\\4286491426.py:47: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  jet = cm.get_cmap('jet', len(unique_keywords))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"global_smart_keywords_map.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x23aa32c2dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate Map 2: Most Frequent Smart Keywords\n",
    "print(\"Processing smart_keywords column...\")\n",
    "skw_results, unique_skws = get_country_top_keywords(df, 'smart_keywords')\n",
    "generate_keyword_map(skw_results, unique_skws, \"Top Smart Keywords\", \"global_smart_keywords_map.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
